{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the keras libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding #To convert an integer to embedding\n",
    "from keras.preprocessing import sequence #To convert a variable length sentence into a prespecified length\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s_jaysetty/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/s_jaysetty/.local/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import the sklearn libraries\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm,grid_search\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nfsroot/data/home/s_jaysetty/sms'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the relevant path in your local machine\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Label                                            Message\n",
      "0       ham                 oh how abt 2 days before Christmas\n",
      "1      info  Welcome to OVATION HOLD R.No. 184, 114, 395, 3...\n",
      "2      info  Thank you for using your ICICI bank CREDITcard...\n",
      "3       ham  schedule a meeting with the entire team in the...\n",
      "4       ham                                Tommy is my brother\n",
      "5      spam  OTP is 817453 for the txn of INR 8262.00 at SP...\n",
      "6       ham                   the meeting is scheduled by john\n",
      "7      spam  Dear customer, We wish you a Merry Christmas. ...\n",
      "8      spam  Delivered: Your package withPawzone Red 1.25 i...\n",
      "9      info  The PNR for your Air India Flt 7I115 for PGH-B...\n",
      "10     info  Bimal Auto Agency : Service of your car KA52C8...\n",
      "11     info  Appointment with Dr Clayton in Pune on 2011-08...\n",
      "12     info  Maha Veer Auto Agency : Service of your car KA...\n",
      "13     spam  Dear AirAsia Customer, flight 5Q658 from RJA s...\n",
      "14     info  Dear Guest, Thanks for choosing Forlini's Rest...\n",
      "15      ham                          I will indeed! What time?\n",
      "16     info  Dear Guest, Thanks for choosing 2nd Avenue Del...\n",
      "17     info  Aryan Auto Agency : Service of your car KA87A3...\n",
      "18     info  Welcome to China Shipbuilding Industry R.No. 4...\n",
      "19      ham                  i will meet you on coming morning\n",
      "20     info  * KSRTC m-Ticket *  from: SRINGERI to: Pune Ps...\n",
      "21     info  PNR:3753775534,TRAIN:23181,DOJ:2013-01-02,AC3,...\n",
      "22      ham                i will meet john adams in bangalore\n",
      "23      ham   I have to catchup for dinner @ nite at his place\n",
      "24      ham                        lets meet 29th after diwali\n",
      "25     info  Repair ref.no for your car is JC53251731 opene...\n",
      "26     spam  Ur transaction on HDFC Bank CREDIT Card ending...\n",
      "27     info  [Cranks] Hi Joseph, your reservation is confir...\n",
      "28     info  Aryan Auto Agency : Service of your car KA17O3...\n",
      "29     spam  Roaming Info: Opt for International Roaming pa...\n",
      "...     ...                                                ...\n",
      "29970   ham                          meet for party over movie\n",
      "29971  info  Hi Customer, Booking ID: WZY37U5. Seats: STAND...\n",
      "29972  info  Dear Guest, Thanks for choosing Tastte!. Order...\n",
      "29973  info  Welcome to us polo R.No. 369, 438 Ch.In 2011-0...\n",
      "29974  info  Thank you for choosing Uber for 2010-01-05 at ...\n",
      "29975  info  YourBus (AGUMBE-KIAL 19:10) left  Jalahalli Cr...\n",
      "29976  info  Repair ref.no for your car is JC45757532 opene...\n",
      "29977   ham                            I'm am kind of busy now\n",
      "29978  spam  Delivered: Your package with 3.3V &amp; 5V Pow...\n",
      "29979  info  * KSRTC m-Ticket *  from: THIRUNALLAR to: PUTT...\n",
      "29980  spam  Delivered: Your package withPrestige PGMFB 800...\n",
      "29981  info  Repair ref.no for your car is JC63858438 opene...\n",
      "29982   ham  What time did you want me to drop you off at t...\n",
      "29983  info  Repair ref.no for your car is JC75514723 opene...\n",
      "29984   ham                                   2nd of September\n",
      "29985  info  [Coffeeshop Company] Hi Mario, your reservatio...\n",
      "29986  info  * KSRTC m-Ticket *  from: SRIKALAHASTI to: ANW...\n",
      "29987   ham          Hi..Please bring 1 ltr bottle of Pepsi ..\n",
      "29988   ham         there is a 40 minutes meeting starting 3pm\n",
      "29989   ham  Call in the hospital number between 5 and 6 an...\n",
      "29990  info  * KSRTC m-Ticket *  from: ALIKE to: Panaji Goa...\n",
      "29991   ham                                   Ok , c u there..\n",
      "29992  info  The PNR for your INDIGO Flt 8R176 for PAT-HJR ...\n",
      "29993  info  [Brewers Fayre] Hi Tristan, your reservation i...\n",
      "29994  spam  Dear AirAsia Customer, flight 2D866 from PAT s...\n",
      "29995   ham                               ok that works for me\n",
      "29996  spam  Delivered: Your package withSri High Quality S...\n",
      "29997   ham                      Let us dine at the Taj on Sat\n",
      "29998  spam  Delivered: Your package withSeCro (Pack of 2) ...\n",
      "29999  spam  OTP is 676348 for the txn of INR 6540.00 at Ar...\n",
      "\n",
      "[30000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "\n",
    "sms_data = pd.read_csv('TRAIN_SMS.csv')\n",
    "print(sms_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple definition to process the words further\n",
    "\n",
    "def message_to_words(raw_message):\n",
    "#     letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_converse) \n",
    "    # 1. Lower case & split  \n",
    "    words = raw_message.lower().split()                             \n",
    "    # 2. Convert stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # 3. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    # 4. Join the words back into one string\n",
    "    return(b\" \".join(meaningful_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the length of the column\n",
    "num_message = sms_data[\"Message\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "([u'oh abt 2 days christmas', u'welcome ovation hold r.no. 184, 114, 395, 378 ch.in 2014-10-21 3:53 ch.out 2014-11-01 12:00.', u'thank using icici bank creditcard ending 5253 rs. 2520.00 alike snapdeal 2013-05-31 21:35'], ['ham', 'info', 'info'])\n"
     ]
    }
   ],
   "source": [
    "print(num_message)\n",
    "print(Message[:3], Label[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty directories\n",
    "\n",
    "Message = []\n",
    "Label = [] # First target\n",
    "\n",
    "for x in range(len(sms_data.Message)):\n",
    "    Message.append(message_to_words(str(sms_data.Message[x]).decode('ascii','ignore')))\n",
    "    Label.append(sms_data.Label[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'00',\n",
       " u'000',\n",
       " u'01',\n",
       " u'02',\n",
       " u'03',\n",
       " u'04',\n",
       " u'05',\n",
       " u'06',\n",
       " u'07',\n",
       " u'08',\n",
       " u'09',\n",
       " u'10',\n",
       " u'100',\n",
       " u'1000',\n",
       " u'10am',\n",
       " u'10hrs',\n",
       " u'10th',\n",
       " u'11',\n",
       " u'11am',\n",
       " u'11hrs',\n",
       " u'12',\n",
       " u'121',\n",
       " u'12hrs',\n",
       " u'12th',\n",
       " u'13',\n",
       " u'13hrs',\n",
       " u'14',\n",
       " u'14hrs',\n",
       " u'14th',\n",
       " u'15',\n",
       " u'15hrs',\n",
       " u'15th',\n",
       " u'16',\n",
       " u'16hrs',\n",
       " u'17',\n",
       " u'17hrs',\n",
       " u'17th',\n",
       " u'18',\n",
       " u'1800',\n",
       " u'18hrs',\n",
       " u'18th',\n",
       " u'19',\n",
       " u'19hrs',\n",
       " u'19th',\n",
       " u'1st',\n",
       " u'20',\n",
       " u'200',\n",
       " u'2009',\n",
       " u'2010',\n",
       " u'2011',\n",
       " u'2012',\n",
       " u'2013',\n",
       " u'2014',\n",
       " u'2015',\n",
       " u'2016',\n",
       " u'2017',\n",
       " u'20hrs',\n",
       " u'20th',\n",
       " u'21',\n",
       " u'21hrs',\n",
       " u'22',\n",
       " u'22hrs',\n",
       " u'23',\n",
       " u'23hrs',\n",
       " u'24',\n",
       " u'24hrs',\n",
       " u'25',\n",
       " u'250',\n",
       " u'25hrs',\n",
       " u'26',\n",
       " u'26hrs',\n",
       " u'27',\n",
       " u'27hrs',\n",
       " u'28',\n",
       " u'28hrs',\n",
       " u'29',\n",
       " u'29hrs',\n",
       " u'2mrw',\n",
       " u'2nd',\n",
       " u'2pm',\n",
       " u'30',\n",
       " u'300',\n",
       " u'30am',\n",
       " u'30hrs',\n",
       " u'30pm',\n",
       " u'31',\n",
       " u'31hrs',\n",
       " u'31st',\n",
       " u'32',\n",
       " u'32hrs',\n",
       " u'33',\n",
       " u'33hrs',\n",
       " u'34',\n",
       " u'34hrs',\n",
       " u'35',\n",
       " u'35hrs',\n",
       " u'36',\n",
       " u'36hrs',\n",
       " u'37',\n",
       " u'37hrs',\n",
       " u'38',\n",
       " u'38hrs',\n",
       " u'39',\n",
       " u'39hrs',\n",
       " u'3g',\n",
       " u'3pm',\n",
       " u'3rd',\n",
       " u'40',\n",
       " u'40hrs',\n",
       " u'41',\n",
       " u'41hrs',\n",
       " u'42',\n",
       " u'42hrs',\n",
       " u'43',\n",
       " u'43hrs',\n",
       " u'44',\n",
       " u'44hrs',\n",
       " u'45',\n",
       " u'45hrs',\n",
       " u'46',\n",
       " u'46hrs',\n",
       " u'47',\n",
       " u'47hrs',\n",
       " u'48',\n",
       " u'48hrs',\n",
       " u'49',\n",
       " u'49hrs',\n",
       " u'4k',\n",
       " u'4pm',\n",
       " u'4th',\n",
       " u'50',\n",
       " u'500',\n",
       " u'50hrs',\n",
       " u'51',\n",
       " u'51hrs',\n",
       " u'52',\n",
       " u'52hrs',\n",
       " u'53',\n",
       " u'53hrs',\n",
       " u'54',\n",
       " u'54hrs',\n",
       " u'55',\n",
       " u'55hrs',\n",
       " u'56',\n",
       " u'56hrs',\n",
       " u'57',\n",
       " u'57hrs',\n",
       " u'58',\n",
       " u'58hrs',\n",
       " u'59',\n",
       " u'5pm',\n",
       " u'5th',\n",
       " u'60',\n",
       " u'61',\n",
       " u'62',\n",
       " u'63',\n",
       " u'64',\n",
       " u'65',\n",
       " u'66',\n",
       " u'67',\n",
       " u'68',\n",
       " u'69',\n",
       " u'6pm',\n",
       " u'70',\n",
       " u'72',\n",
       " u'73',\n",
       " u'74',\n",
       " u'75',\n",
       " u'76',\n",
       " u'77',\n",
       " u'78',\n",
       " u'7967',\n",
       " u'7pm',\n",
       " u'80',\n",
       " u'8066',\n",
       " u'81',\n",
       " u'82',\n",
       " u'85',\n",
       " u'86',\n",
       " u'87',\n",
       " u'88',\n",
       " u'8pm',\n",
       " u'8th',\n",
       " u'9pm',\n",
       " u'a10',\n",
       " u'a11',\n",
       " u'a12',\n",
       " u'a13',\n",
       " u'a14',\n",
       " u'a15',\n",
       " u'a16',\n",
       " u'a17',\n",
       " u'aaron',\n",
       " u'abhanshu',\n",
       " u'abt',\n",
       " u'ac',\n",
       " u'ac3',\n",
       " u'account',\n",
       " u'ad',\n",
       " u'add',\n",
       " u'added',\n",
       " u'adrian',\n",
       " u'afternoon',\n",
       " u'again',\n",
       " u'agara',\n",
       " u'age',\n",
       " u'agency',\n",
       " u'agr',\n",
       " u'agumbe',\n",
       " u'agx',\n",
       " u'air',\n",
       " u'airasia',\n",
       " u'aircel',\n",
       " u'airport',\n",
       " u'airporttaxi',\n",
       " u'airtel',\n",
       " u'airways',\n",
       " u'ajl',\n",
       " u'akd',\n",
       " u'alex',\n",
       " u'alexander',\n",
       " u'alice',\n",
       " u'alike',\n",
       " u'along',\n",
       " u'already',\n",
       " u'alright',\n",
       " u'also',\n",
       " u'am',\n",
       " u'amazon',\n",
       " u'amd',\n",
       " u'amount',\n",
       " u'amp',\n",
       " u'amt',\n",
       " u'anand',\n",
       " u'andy',\n",
       " u'anjan',\n",
       " u'anniversary',\n",
       " u'anwatti',\n",
       " u'anyone',\n",
       " u'app',\n",
       " u'apply',\n",
       " u'appointment',\n",
       " u'approx',\n",
       " u'appt',\n",
       " u'apr',\n",
       " u'april',\n",
       " u'arcade',\n",
       " u'arena',\n",
       " u'around',\n",
       " u'aryan',\n",
       " u'ask',\n",
       " u'askme',\n",
       " u'assigned',\n",
       " u'assistance',\n",
       " u'at',\n",
       " u'atq',\n",
       " u'attend',\n",
       " u'attending',\n",
       " u'attibele',\n",
       " u'aug',\n",
       " u'auto',\n",
       " u'avail',\n",
       " u'available',\n",
       " u'avbl',\n",
       " u'avl',\n",
       " u'axis',\n",
       " u'b1',\n",
       " u'b2',\n",
       " u'b3',\n",
       " u'b4',\n",
       " u'b5',\n",
       " u'b6',\n",
       " u'b7',\n",
       " u'b8',\n",
       " u'b9',\n",
       " u'baby',\n",
       " u'back',\n",
       " u'bad',\n",
       " u'bag',\n",
       " u'bajaj',\n",
       " u'bal',\n",
       " u'balance',\n",
       " u'banashankari',\n",
       " u'banaswadi',\n",
       " u'bangalore',\n",
       " u'bank',\n",
       " u'banking',\n",
       " u'bannerghatta',\n",
       " u'bar',\n",
       " u'basis',\n",
       " u'bazaar',\n",
       " u'bbi',\n",
       " u'bda',\n",
       " u'bdq',\n",
       " u'beach',\n",
       " u'belagavi',\n",
       " u'bellandur',\n",
       " u'bengaluru',\n",
       " u'benjamin',\n",
       " u'bep',\n",
       " u'best',\n",
       " u'beyond',\n",
       " u'bhj',\n",
       " u'bho',\n",
       " u'bhu',\n",
       " u'big',\n",
       " u'bill',\n",
       " u'bills',\n",
       " u'bimal',\n",
       " u'binnypet',\n",
       " u'birthday',\n",
       " u'bit',\n",
       " u'bk',\n",
       " u'bkb',\n",
       " u'black',\n",
       " u'blessed',\n",
       " u'blr',\n",
       " u'blue',\n",
       " u'boarding',\n",
       " u'boardingpt',\n",
       " u'bom',\n",
       " u'bommanahalli',\n",
       " u'book',\n",
       " u'booking',\n",
       " u'box8',\n",
       " u'brandon',\n",
       " u'brayden',\n",
       " u'breakfast',\n",
       " u'brian',\n",
       " u'bridge',\n",
       " u'brigade',\n",
       " u'bring',\n",
       " u'broadband',\n",
       " u'brookefield',\n",
       " u'brother',\n",
       " u'bryce',\n",
       " u'bsnl',\n",
       " u'bucks',\n",
       " u'buddy',\n",
       " u'bup',\n",
       " u'burger',\n",
       " u'bus',\n",
       " u'busy',\n",
       " u'buy',\n",
       " u'buzz',\n",
       " u'by',\n",
       " u'bz',\n",
       " u'cab',\n",
       " u'cafe',\n",
       " u'call',\n",
       " u'called',\n",
       " u'calls',\n",
       " u'camden',\n",
       " u'can',\n",
       " u'cancelled',\n",
       " u'cannot',\n",
       " u'car',\n",
       " u'card',\n",
       " u'cards',\n",
       " u'care',\n",
       " u'carry',\n",
       " u'case',\n",
       " u'cash',\n",
       " u'cashback',\n",
       " u'catch',\n",
       " u'cauvery',\n",
       " u'cbfwh1e',\n",
       " u'cc',\n",
       " u'ccd',\n",
       " u'ccj',\n",
       " u'ccu',\n",
       " u'cdp',\n",
       " u'cell',\n",
       " u'center',\n",
       " u'central',\n",
       " u'centre',\n",
       " u'ch',\n",
       " u'chair',\n",
       " u'chamarajpet',\n",
       " u'channarayapatna',\n",
       " u'charles',\n",
       " u'chase',\n",
       " u'chat',\n",
       " u'chaturthi',\n",
       " u'chauffer',\n",
       " u'check',\n",
       " u'chennai',\n",
       " u'chicken',\n",
       " u'chikkamagaluru',\n",
       " u'children',\n",
       " u'chitradurga',\n",
       " u'chk',\n",
       " u'choosing',\n",
       " u'christmas',\n",
       " u'chuseok',\n",
       " u'cinema',\n",
       " u'cinemas',\n",
       " u'cinepolis',\n",
       " u'circle',\n",
       " u'citibank',\n",
       " u'city',\n",
       " u'cjb',\n",
       " u'class',\n",
       " u'clayton',\n",
       " u'click',\n",
       " u'clinic',\n",
       " u'clock',\n",
       " u'closes1',\n",
       " u'closes10',\n",
       " u'closes11',\n",
       " u'closes12',\n",
       " u'closes13',\n",
       " u'closes14',\n",
       " u'closes15',\n",
       " u'closes16',\n",
       " u'closes17',\n",
       " u'closes18',\n",
       " u'closes19',\n",
       " u'closes2',\n",
       " u'closes20',\n",
       " u'closes21',\n",
       " u'closes22',\n",
       " u'closes23',\n",
       " u'closes25',\n",
       " u'closes26',\n",
       " u'closes27',\n",
       " u'closes28',\n",
       " u'closes29',\n",
       " u'closes3',\n",
       " u'closes4',\n",
       " u'closes5',\n",
       " u'closes6',\n",
       " u'closes7',\n",
       " u'closes8',\n",
       " u'closes9',\n",
       " u'clothes',\n",
       " u'club',\n",
       " u'cm',\n",
       " u'co',\n",
       " u'code',\n",
       " u'cody',\n",
       " u'coffee',\n",
       " u'coimbatore',\n",
       " u'cok',\n",
       " u'colby',\n",
       " u'cole',\n",
       " u'collection',\n",
       " u'college',\n",
       " u'collin',\n",
       " u'com',\n",
       " u'come',\n",
       " u'coming',\n",
       " u'company',\n",
       " u'complex',\n",
       " u'concerned',\n",
       " u'confirm',\n",
       " u'confirmed',\n",
       " u'conner',\n",
       " u'contact',\n",
       " u'contact_name',\n",
       " u'cool',\n",
       " u'coonur',\n",
       " u'cooper',\n",
       " u'coroporation',\n",
       " u'could',\n",
       " u'course',\n",
       " u'cover',\n",
       " u'cpn',\n",
       " u'cream',\n",
       " u'create',\n",
       " u'credit',\n",
       " u'creditcard',\n",
       " u'credited',\n",
       " u'cross',\n",
       " u'curr',\n",
       " u'customer',\n",
       " u'd16',\n",
       " u'd17',\n",
       " u'dae',\n",
       " u'daily',\n",
       " u'dairy',\n",
       " u'dakota',\n",
       " u'damian',\n",
       " u'dance',\n",
       " u'daniel',\n",
       " u'data',\n",
       " u'date',\n",
       " u'date_sent',\n",
       " u'dated',\n",
       " u'davanegere',\n",
       " u'day',\n",
       " u'days',\n",
       " u'dc',\n",
       " u'de',\n",
       " u'dear',\n",
       " u'debit',\n",
       " u'debitcard',\n",
       " u'debited',\n",
       " u'dec',\n",
       " u'december',\n",
       " u'ded',\n",
       " u'defaulting',\n",
       " u'del',\n",
       " u'delayed',\n",
       " u'delhi',\n",
       " u'delivered',\n",
       " u'delivery',\n",
       " u'dentist',\n",
       " u'dep',\n",
       " u'departure',\n",
       " u'deptime',\n",
       " u'details',\n",
       " u'dharmastala',\n",
       " u'dhm',\n",
       " u'dial',\n",
       " u'dib',\n",
       " u'digital',\n",
       " u'din',\n",
       " u'diner',\n",
       " u'dinner',\n",
       " u'disconnected',\n",
       " u'discount',\n",
       " u'discover',\n",
       " u'discuss',\n",
       " u'discussion',\n",
       " u'diu',\n",
       " u'diwali',\n",
       " u'dmu',\n",
       " u'docomo',\n",
       " u'doctor',\n",
       " u'dog',\n",
       " u'doj',\n",
       " u'dominick',\n",
       " u'dominos',\n",
       " u'domlur',\n",
       " u'don',\n",
       " u'done',\n",
       " u'dont',\n",
       " u'donuts',\n",
       " u'download',\n",
       " u'downtown',\n",
       " u'dr',\n",
       " u'drop',\n",
       " u'due',\n",
       " u'dusshera',\n",
       " u'dylan',\n",
       " u'early',\n",
       " u'east',\n",
       " u'eat',\n",
       " u'ebay',\n",
       " u'edwin',\n",
       " u'eid',\n",
       " u'electricity',\n",
       " u'electronic',\n",
       " u'elements',\n",
       " u'eli',\n",
       " u'elijah',\n",
       " u'email',\n",
       " u'emergency',\n",
       " u'emi',\n",
       " u'end',\n",
       " u'ending',\n",
       " u'engagement',\n",
       " u'enjoy',\n",
       " u'enquire',\n",
       " u'ernakulam',\n",
       " u'est',\n",
       " u'etd',\n",
       " u'eve',\n",
       " u'evening',\n",
       " u'event',\n",
       " u'every',\n",
       " u'everyday',\n",
       " u'exclusive',\n",
       " u'experience',\n",
       " u'extra',\n",
       " u'f1',\n",
       " u'f2',\n",
       " u'f3',\n",
       " u'f4',\n",
       " u'f5',\n",
       " u'f6',\n",
       " u'f7',\n",
       " u'f8',\n",
       " u'f9',\n",
       " u'falls',\n",
       " u'family',\n",
       " u'fare',\n",
       " u'fb',\n",
       " u'fc',\n",
       " u'feb',\n",
       " u'field',\n",
       " u'find',\n",
       " u'fine',\n",
       " u'finserv',\n",
       " u'first',\n",
       " u'fitr',\n",
       " u'fixedline',\n",
       " u'flat',\n",
       " u'flight',\n",
       " u'flipkart',\n",
       " u'flt',\n",
       " u'flyover',\n",
       " u'food',\n",
       " u'for',\n",
       " u'forget',\n",
       " u'forgot',\n",
       " u'forum',\n",
       " u'free',\n",
       " u'fresh',\n",
       " u'friday',\n",
       " u'friends',\n",
       " u'from',\n",
       " u'fun',\n",
       " u'galaxy',\n",
       " u'game',\n",
       " u'gandhi',\n",
       " u'gandhinagar',\n",
       " u'gandinagar',\n",
       " u'ganesh',\n",
       " u'garuda',\n",
       " u'gate',\n",
       " u'gau',\n",
       " u'gay',\n",
       " u'gb',\n",
       " u'generated',\n",
       " u'george',\n",
       " u'gerardo',\n",
       " u'get',\n",
       " u'gift',\n",
       " u'giovanni',\n",
       " u'girl',\n",
       " u'give',\n",
       " u'gl',\n",
       " u'glass',\n",
       " u'go',\n",
       " u'goa',\n",
       " u'going',\n",
       " u'gold',\n",
       " u'gonna',\n",
       " u'goo',\n",
       " u'good',\n",
       " u'gop',\n",
       " u'gopalan',\n",
       " u'goragunte',\n",
       " u'got',\n",
       " u'gotta',\n",
       " u'grab',\n",
       " u'grand',\n",
       " u'grant',\n",
       " u'great',\n",
       " u'greet',\n",
       " u'greetings',\n",
       " u'griffin',\n",
       " u'grill',\n",
       " u'group',\n",
       " u'gt',\n",
       " u'guest',\n",
       " u'gwl',\n",
       " u'gym',\n",
       " u'half',\n",
       " u'hall',\n",
       " u'happy',\n",
       " u'harrison',\n",
       " u'harry',\n",
       " u'hassan',\n",
       " u'hbx',\n",
       " u'hdfc',\n",
       " u'hdfcbank',\n",
       " u'health',\n",
       " u'heavy',\n",
       " u'hebbal',\n",
       " u'hello',\n",
       " u'help',\n",
       " u'henry',\n",
       " u'here',\n",
       " u'hey',\n",
       " u'hi',\n",
       " u'high',\n",
       " u'hiriyur',\n",
       " u'hjr',\n",
       " u'hold',\n",
       " u'holi',\n",
       " u'home',\n",
       " u'honey',\n",
       " u'hoodi',\n",
       " u'hospital',\n",
       " u'hosur',\n",
       " u'hot',\n",
       " u'hotel',\n",
       " u'hour',\n",
       " u'hours',\n",
       " u'house',\n",
       " u'hrs',\n",
       " u'hsr',\n",
       " u'http',\n",
       " u'https',\n",
       " u'hubli',\n",
       " u'hundimala',\n",
       " u'hurry',\n",
       " u'hut',\n",
       " u'hv',\n",
       " u'hyd',\n",
       " u'hyderabad',\n",
       " u'hyundai',\n",
       " u'i13',\n",
       " u'ice',\n",
       " u'icici',\n",
       " u'id',\n",
       " u'idbi',\n",
       " u'idea',\n",
       " u'idr',\n",
       " u'imf',\n",
       " u'imps',\n",
       " u'in',\n",
       " u'independence',\n",
       " u'india',\n",
       " u'indigo',\n",
       " u'indira',\n",
       " u'info',\n",
       " u'inform',\n",
       " u'initiated',\n",
       " u'innovative',\n",
       " u'inox',\n",
       " u'inr',\n",
       " u'inr1',\n",
       " u'ins',\n",
       " u'intelligence',\n",
       " u'interest',\n",
       " u'international',\n",
       " u'internet',\n",
       " u'interview',\n",
       " u'invite',\n",
       " u'ips',\n",
       " u'isaac',\n",
       " u'isaiah',\n",
       " u'isk',\n",
       " u'israel',\n",
       " u'it',\n",
       " u'item',\n",
       " u'items',\n",
       " u'itemswas',\n",
       " u'itemwas',\n",
       " u'ixe',\n",
       " u'ixm',\n",
       " u'j10',\n",
       " u'j5',\n",
       " u'j6',\n",
       " u'j7',\n",
       " u'j8',\n",
       " u'j9',\n",
       " u'jabong',\n",
       " u'jack',\n",
       " u'jai',\n",
       " u'jalahalli',\n",
       " u'james',\n",
       " u'jan',\n",
       " u'janmashtami',\n",
       " u'january',\n",
       " u'jared',\n",
       " u'jason',\n",
       " u'jaxon',\n",
       " u'jaya',\n",
       " u'jayanagar',\n",
       " u'jayanti',\n",
       " u'jayden',\n",
       " u'jdh',\n",
       " u'jeremiah',\n",
       " u'jesus',\n",
       " u'jet',\n",
       " u'jga',\n",
       " u'jgb',\n",
       " u'jio',\n",
       " u'jlr',\n",
       " u'jnydate',\n",
       " u'john',\n",
       " u'johnny',\n",
       " u'join',\n",
       " u'jonathan',\n",
       " u'jordan',\n",
       " u'jose',\n",
       " u'joseph',\n",
       " u'josue',\n",
       " u'journey',\n",
       " u'jp',\n",
       " u'jrh',\n",
       " u'jsa',\n",
       " u'jul',\n",
       " u'julian',\n",
       " u'july',\n",
       " u'jun',\n",
       " u'junction',\n",
       " u'june',\n",
       " u'jungle',\n",
       " u'kailash',\n",
       " u'kalasipalayam',\n",
       " u'kannur',\n",
       " u'karaikudi',\n",
       " u'kasargod',\n",
       " u'kasturinagar',\n",
       " u'keep',\n",
       " u'kenneth',\n",
       " u'kevin',\n",
       " u'kg',\n",
       " u'kial',\n",
       " u'king',\n",
       " u'klh',\n",
       " u'know',\n",
       " u'kodaikanal',\n",
       " u'kolar',\n",
       " u'kolhapur',\n",
       " u'kottayam',\n",
       " u'kozhikode',\n",
       " u'ksrtc',\n",
       " u'ktu',\n",
       " u'kumbakonam',\n",
       " u'kundapura',\n",
       " u'la',\n",
       " u'lakshmi',\n",
       " u'land',\n",
       " u'landen',\n",
       " u'lane',\n",
       " u'last',\n",
       " u'late',\n",
       " u'later',\n",
       " u'latest',\n",
       " u'laundry',\n",
       " u'lawyer',\n",
       " u'layout',\n",
       " u'lda',\n",
       " u'leave',\n",
       " u'left',\n",
       " u'let',\n",
       " u'lets',\n",
       " u'levi',\n",
       " u'library',\n",
       " u'lido',\n",
       " u'life',\n",
       " u'lights',\n",
       " u'like',\n",
       " u'limit',\n",
       " u'little',\n",
       " u'lko',\n",
       " u'll',\n",
       " u'loan',\n",
       " u'local',\n",
       " u'location',\n",
       " u'locked',\n",
       " u'london',\n",
       " u'long',\n",
       " u'looking',\n",
       " u'love',\n",
       " u'ltd',\n",
       " u'lucas',\n",
       " u'luh',\n",
       " u'lunch',\n",
       " u'ly',\n",
       " u'm12',\n",
       " u'm6',\n",
       " u'm7',\n",
       " u'made',\n",
       " u'madikeri',\n",
       " u'madras',\n",
       " u'madurai',\n",
       " u'magadi',\n",
       " u'magrath',\n",
       " u'maha',\n",
       " u'mahadevapura',\n",
       " u'make',\n",
       " u'makemytrip',\n",
       " u'mall',\n",
       " u'mandya',\n",
       " u'mangaluru',\n",
       " u'manipal',\n",
       " u'mantralaya',\n",
       " u'mantri',\n",
       " u'mar',\n",
       " u'marathahalli',\n",
       " u'march',\n",
       " u'marcos',\n",
       " u'mark',\n",
       " u'market',\n",
       " u'marriage',\n",
       " u'martin',\n",
       " u'maruti',\n",
       " u'mary',\n",
       " u'max',\n",
       " u'maximum',\n",
       " u'maxwell',\n",
       " u'may',\n",
       " u'me',\n",
       " u'meenakshi',\n",
       " u'meet',\n",
       " u'meeting',\n",
       " u'member',\n",
       " u'men',\n",
       " u'mercara',\n",
       " u'merry',\n",
       " u'meru',\n",
       " u'message',\n",
       " u'messages',\n",
       " u'mg',\n",
       " u'michael',\n",
       " u'might',\n",
       " u'mile',\n",
       " u'milk',\n",
       " u'min',\n",
       " u'miniplex',\n",
       " u'mins',\n",
       " u'minutes',\n",
       " u'miss',\n",
       " u'missed',\n",
       " u'mobile',\n",
       " u'mom',\n",
       " u'mon',\n",
       " u'monday',\n",
       " u'money',\n",
       " u'month',\n",
       " u'months',\n",
       " u'more',\n",
       " u'morning',\n",
       " u'movie',\n",
       " u'mr',\n",
       " u'mrs',\n",
       " u'msr',\n",
       " u'much',\n",
       " u'muharram',\n",
       " u'multiplex',\n",
       " u'mumbai',\n",
       " u'must',\n",
       " u'mwgvjp',\n",
       " u'myantra',\n",
       " u'myq',\n",
       " u'mysore',\n",
       " u'mysuru',\n",
       " u'mzu',\n",
       " u'n16',\n",
       " u'nag',\n",
       " u'nagar',\n",
       " u'name',\n",
       " u'national',\n",
       " u'navami',\n",
       " u'ndc',\n",
       " u'near',\n",
       " u'need',\n",
       " u'neighbourhood',\n",
       " u'nellore',\n",
       " u'net',\n",
       " u'network',\n",
       " u'never',\n",
       " u'new',\n",
       " u'next',\n",
       " u'nicolas',\n",
       " u'night',\n",
       " u'nishu',\n",
       " u'nmb',\n",
       " u'no',\n",
       " u'non',\n",
       " u'noon',\n",
       " u'northstar',\n",
       " u'note',\n",
       " u'nov',\n",
       " u'now',\n",
       " u'null',\n",
       " u'number',\n",
       " u'oct',\n",
       " u'october',\n",
       " u'off',\n",
       " u'offer',\n",
       " u'offers',\n",
       " u'office',\n",
       " u'oh',\n",
       " u'ohh',\n",
       " u'ok',\n",
       " u'okay',\n",
       " u'ola',\n",
       " u'old',\n",
       " u'on',\n",
       " u'onam',\n",
       " u'one',\n",
       " u'online',\n",
       " u'only',\n",
       " u'ooty',\n",
       " u'open',\n",
       " u'opened',\n",
       " u'operator',\n",
       " u'opt',\n",
       " u'order',\n",
       " u'orders',\n",
       " u'orion',\n",
       " u'otp',\n",
       " u'out',\n",
       " u'pab',\n",
       " u'pack',\n",
       " u'package',\n",
       " u'paid',\n",
       " u'palakkad',\n",
       " u'palya',\n",
       " u'panaji',\n",
       " u'panchami',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create word frequency matrix for every Message, \n",
    "#Apply Tfidf vectorizer-extracting upto 1400 features\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\\\n",
    "                             tokenizer = None,\\\n",
    "                             preprocessor = None,\n",
    "                             ngram_range=(1,1), # We are only interested in uni grams\n",
    "                             max_features=1400, # Limits to 1400 features\n",
    "                             stop_words = None) \n",
    "\n",
    "data_features = vectorizer.fit_transform(Message)\n",
    "type(data_features)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 30000)\n",
      "['ham' 'info' 'spam']\n"
     ]
    }
   ],
   "source": [
    "print(len(Message), len(Label)) #30000 obs\n",
    "print(np.unique(Label)) # 3 different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data_features to array\n",
    "\n",
    "# data features for categories\n",
    "data_features = data_features.toarray()\n",
    "data_features = pd.DataFrame(data_features)\n",
    "data_features[\"Label\"] = sms_data[\"Label\"]\n",
    "data_features = data_features.sample(frac =1)\n",
    "\n",
    "# #data features for sub_categories\n",
    "\n",
    "# data_features[\"sub_categories\"] = health_data[\"sub_categories\"]\n",
    "# data_features = data_features.sample(frac =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0  1  2  3  4  5  6  7  8  9  ...    1391  1392  1393  1394  1395  \\\n",
      "1252   0  0  0  0  0  0  0  0  0  0  ...       0     0     0     0     0   \n",
      "10444  0  0  0  0  0  0  0  0  0  0  ...       0     0     0     0     0   \n",
      "8994   1  0  0  0  0  0  0  0  0  0  ...       0     0     0     0     0   \n",
      "\n",
      "       1396  1397  1398  1399  Label  \n",
      "1252      0     0     0     0   spam  \n",
      "10444     0     0     0     0    ham  \n",
      "8994      0     0     0     0   spam  \n",
      "\n",
      "[3 rows x 1401 columns]\n",
      "(30000, 1401)\n"
     ]
    }
   ],
   "source": [
    "# Verify the shape and head of data_features\n",
    "print(data_features[:3])\n",
    "print(data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sms_data))\n",
    "print(type(data_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfsroot/data/home/s_jaysetty/.local/lib/python2.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Split into 60% train, 20% val and 20% test\n",
    "\n",
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.ix[perm[:train_end]]\n",
    "    validate = df.ix[perm[train_end:validate_end]]\n",
    "    test = df.ix[perm[validate_end:]]\n",
    "    return train, validate, test\n",
    "\n",
    "#First split\n",
    "train, validate, test = train_validate_test_split(data_features) \n",
    "cols = [col for col in data_features.columns if col not in [\"Label\"]]\n",
    "\n",
    "train.x = train[cols]\n",
    "train.y = train[\"Label\"]\n",
    "# train.z = train[\"sub_categories\"]\n",
    "\n",
    "validate.x = validate[cols]\n",
    "validate.y = validate[\"Label\"]\n",
    "# validate.z = validate[\"sub_categories\"]\n",
    "\n",
    "test.x = test[cols]\n",
    "test.y = test[\"Label\"]\n",
    "# test.z = test[\"sub_categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((18000, 1400), (18000,))\n",
      "((6000, 1400), (6000,))\n",
      "((6000, 1400), (6000,))\n",
      "       0     1     2     3     4     5     6     7     8     9     ...   1390  \\\n",
      "15559     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
      "11214     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
      "14976     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
      "\n",
      "       1391  1392  1393  1394  1395  1396  1397  1398  1399  \n",
      "15559     0     0     0     0     0     0     0     0     0  \n",
      "11214     0     0     0     0     0     0     0     0     0  \n",
      "14976     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[3 rows x 1400 columns]\n",
      "15559    ham\n",
      "11214    ham\n",
      "14976    ham\n",
      "Name: Label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Verify the shape of train.x, train.y\n",
    "print(train.x.shape, train.y.shape)\n",
    "print(validate.x.shape, validate.y.shape)\n",
    "print(test.x.shape, test.y.shape)\n",
    "# print(health_data.converse.map(len).max())\n",
    "\n",
    "print(train.x[:3])\n",
    "print(train.y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`sequences` must be a list of iterables. Found non-iterable: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9ef5e84024bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_review_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;31m# This should be actually health_data.converse.map(len).max()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/preprocessing/sequence.pyc\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             raise ValueError('`sequences` must be a list of iterables. '\n\u001b[0;32m---> 42\u001b[0;31m                              'Found non-iterable: ' + str(x))\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `sequences` must be a list of iterables. Found non-iterable: 0"
     ]
    }
   ],
   "source": [
    "# Truncate and Pad input sequences\n",
    "\n",
    "max_review_length = 500 # This should be actually health_data.converse.map(len).max()\n",
    "train.x = sequence.pad_sequences(train.x, maxlen=max_review_length)\n",
    "test.x = sequence.pad_sequences(test.x, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's inspect the first two sentences of train and their classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1894,   96,   38],\n",
       "       [   0, 2638,    0],\n",
       "       [  32,    8, 1294]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Naive Bayes Model\n",
    "\n",
    "Mnb = MultinomialNB()\n",
    "Mnb.fit(train.x, train.y)\n",
    "preds_NB = Mnb.predict(test.x)\n",
    "confusion_matrix(test.y,preds_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.97099999999999997, 0.9679800139378042)\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Accuracy and recall\n",
    "\n",
    "accuracy = metrics.accuracy_score(test.y,preds_NB)\n",
    "recall = metrics.recall_score(test.y,preds_NB, average = 'macro')\n",
    "print(accuracy, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the same sentences after padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'info' 'spam']\n"
     ]
    }
   ],
   "source": [
    "# Basic checks so that train.x, train.y,..,test.y are all in the same shape\n",
    "print(np.unique(train.y))\n",
    "# print(np.unique(train.z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert the train.y into simple categorical variables 0,1,2\n",
    "train.y = pd.Categorical(train.y)\n",
    "train.y.codes\n",
    "train.y = to_categorical(np.asarray(train.y.codes))\n",
    "\n",
    "# Similarly convert the test.y into simple categorical variables 0,1,2\n",
    "test.y = pd.Categorical(test.y)\n",
    "test.y.codes\n",
    "test.y = to_categorical(np.asarray(test.y.codes))\n",
    "\n",
    "# Similarly convert the val.y into simple categorical variables 0,1,2\n",
    "validate.y = pd.Categorical(validate.y)\n",
    "validate.y.codes\n",
    "validate.y = to_categorical(np.asarray(validate.y.codes))\n",
    "\n",
    "print(train.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.y.shape)\n",
    "# print(train.z[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train.y into simple categorical variables from 0,1,2,..,6\n",
    "train.z = pd.Categorical(train.z)\n",
    "train.z.codes\n",
    "train.z = to_categorical(np.asarray(train.z.codes))\n",
    "\n",
    "# Similarly convert the test.y into simple categorical variables from 0,1,2,..,6\n",
    "test.z = pd.Categorical(test.z)\n",
    "test.z.codes\n",
    "test.z = to_categorical(np.asarray(test.z.codes))\n",
    "\n",
    "# Similarly convert the val.y into simple categorical variables from 0,1,2,..,6\n",
    "validate.z = pd.Categorical(validate.z)\n",
    "validate.z.codes\n",
    "validate.z = to_categorical(np.asarray(validate.z.codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((18000, 1400), (18000, 3))\n",
      "(<class 'pandas.core.frame.DataFrame'>, <type 'numpy.ndarray'>)\n",
      "((6000, 1400), (6000, 3))\n"
     ]
    }
   ],
   "source": [
    "# Verify the shape of train.x and train.y before feeding into the model\n",
    "print(train.x.shape, train.y.shape)\n",
    "print(type(train.x), type(train.y))\n",
    "\n",
    "print(validate.x.shape, validate.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custome \"Recall\" error function in Keras backend\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    PP = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = TP / (PP + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               179328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 179,715\n",
      "Trainable params: 179,715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Simple MLP, SGD and Dropout [6 categories]\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=1400, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])\n",
    "\n",
    "# sgd = SGD(lr=0.04, decay=1e-6, momentum=0.6, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) 0.82% test accuracy\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      "3s - loss: 0.1138 - acc: 0.9681 - recall: 0.9367 - val_loss: 0.0177 - val_acc: 0.9952 - val_recall: 0.9948\n",
      "Epoch 2/15\n",
      "2s - loss: 0.0131 - acc: 0.9968 - recall: 0.9968 - val_loss: 0.0134 - val_acc: 0.9960 - val_recall: 0.9960\n",
      "Epoch 3/15\n",
      "1s - loss: 0.0071 - acc: 0.9982 - recall: 0.9982 - val_loss: 0.0138 - val_acc: 0.9958 - val_recall: 0.9957\n",
      "Epoch 4/15\n",
      "1s - loss: 0.0047 - acc: 0.9988 - recall: 0.9988 - val_loss: 0.0138 - val_acc: 0.9962 - val_recall: 0.9962\n",
      "Epoch 5/15\n",
      "1s - loss: 0.0033 - acc: 0.9990 - recall: 0.9990 - val_loss: 0.0151 - val_acc: 0.9957 - val_recall: 0.9957\n",
      "Epoch 6/15\n",
      "1s - loss: 0.0025 - acc: 0.9994 - recall: 0.9994 - val_loss: 0.0159 - val_acc: 0.9957 - val_recall: 0.9957\n",
      "Epoch 7/15\n",
      "1s - loss: 0.0022 - acc: 0.9996 - recall: 0.9996 - val_loss: 0.0173 - val_acc: 0.9957 - val_recall: 0.9957\n",
      "Epoch 8/15\n",
      "1s - loss: 0.0016 - acc: 0.9996 - recall: 0.9996 - val_loss: 0.0171 - val_acc: 0.9960 - val_recall: 0.9960\n",
      "Epoch 9/15\n",
      "1s - loss: 0.0012 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0195 - val_acc: 0.9953 - val_recall: 0.9953\n",
      "Epoch 10/15\n",
      "2s - loss: 0.0011 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0188 - val_acc: 0.9955 - val_recall: 0.9955\n",
      "Epoch 11/15\n",
      "2s - loss: 0.0011 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0202 - val_acc: 0.9957 - val_recall: 0.9957\n",
      "Epoch 12/15\n",
      "2s - loss: 0.0010 - acc: 0.9999 - recall: 0.9999 - val_loss: 0.0204 - val_acc: 0.9952 - val_recall: 0.9952\n",
      "Epoch 13/15\n",
      "1s - loss: 7.8273e-04 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0240 - val_acc: 0.9952 - val_recall: 0.9950\n",
      "Epoch 14/15\n",
      "1s - loss: 0.0011 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0226 - val_acc: 0.9952 - val_recall: 0.9952\n",
      "Epoch 15/15\n",
      "1s - loss: 6.3139e-04 - acc: 0.9999 - recall: 0.9999 - val_loss: 0.0221 - val_acc: 0.9955 - val_recall: 0.9955\n",
      "Accurcay_mlp_cat: 99.43%\n",
      "Recall_mlp_cat: 99.43%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model for y = categories\n",
    "model.fit(np.array(train.x), train.y, nb_epoch=15, batch_size=32, verbose=2, validation_data=(np.array(validate.x), validate.y))\n",
    "\n",
    "scores_mlp = model.evaluate(np.array(test.x), test.y, verbose=0)\n",
    "print(\"Accurcay_mlp_cat: %.2f%%\" % (scores_mlp[1]*100))\n",
    "print(\"Recall_mlp_cat: %.2f%%\" % (scores_mlp[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 128)               640128    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 21)                2709      \n",
      "=================================================================\n",
      "Total params: 642,837\n",
      "Trainable params: 642,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 36658 samples, validate on 9165 samples\n",
      "Epoch 1/20\n",
      "5s - loss: 1.1832 - acc: 0.6602 - recall: 0.4836 - val_loss: 0.8797 - val_acc: 0.7200 - val_recall: 0.6123\n",
      "Epoch 2/20\n",
      "5s - loss: 0.8057 - acc: 0.7411 - recall: 0.6425 - val_loss: 0.8467 - val_acc: 0.7261 - val_recall: 0.6468\n",
      "Epoch 3/20\n",
      "5s - loss: 0.6784 - acc: 0.7773 - recall: 0.6972 - val_loss: 0.8522 - val_acc: 0.7276 - val_recall: 0.6670\n",
      "Epoch 4/20\n",
      "5s - loss: 0.5853 - acc: 0.8036 - recall: 0.7413 - val_loss: 0.8857 - val_acc: 0.7273 - val_recall: 0.6771\n",
      "Epoch 5/20\n",
      "5s - loss: 0.5142 - acc: 0.8248 - recall: 0.7697 - val_loss: 0.9232 - val_acc: 0.7215 - val_recall: 0.6840\n",
      "Epoch 6/20\n",
      "5s - loss: 0.4525 - acc: 0.8437 - recall: 0.7992 - val_loss: 0.9858 - val_acc: 0.7236 - val_recall: 0.6925\n",
      "Epoch 7/20\n",
      "5s - loss: 0.4046 - acc: 0.8589 - recall: 0.8203 - val_loss: 1.0298 - val_acc: 0.7197 - val_recall: 0.6909\n",
      "Epoch 8/20\n",
      "5s - loss: 0.3684 - acc: 0.8730 - recall: 0.8384 - val_loss: 1.0615 - val_acc: 0.7198 - val_recall: 0.6918\n",
      "Epoch 9/20\n",
      "5s - loss: 0.3312 - acc: 0.8841 - recall: 0.8540 - val_loss: 1.1280 - val_acc: 0.7185 - val_recall: 0.6948\n",
      "Epoch 10/20\n",
      "5s - loss: 0.2992 - acc: 0.8963 - recall: 0.8713 - val_loss: 1.1821 - val_acc: 0.7143 - val_recall: 0.6942\n",
      "Epoch 11/20\n",
      "5s - loss: 0.2785 - acc: 0.9026 - recall: 0.8798 - val_loss: 1.2422 - val_acc: 0.7137 - val_recall: 0.6968\n",
      "Epoch 12/20\n",
      "5s - loss: 0.2609 - acc: 0.9095 - recall: 0.8883 - val_loss: 1.2745 - val_acc: 0.7169 - val_recall: 0.7005\n",
      "Epoch 13/20\n",
      "5s - loss: 0.2413 - acc: 0.9148 - recall: 0.8954 - val_loss: 1.3294 - val_acc: 0.7153 - val_recall: 0.7006\n",
      "Epoch 14/20\n",
      "5s - loss: 0.2234 - acc: 0.9227 - recall: 0.9062 - val_loss: 1.3622 - val_acc: 0.7134 - val_recall: 0.6973\n",
      "Epoch 15/20\n",
      "5s - loss: 0.2103 - acc: 0.9255 - recall: 0.9107 - val_loss: 1.4155 - val_acc: 0.7131 - val_recall: 0.7015\n",
      "Epoch 16/20\n",
      "5s - loss: 0.1997 - acc: 0.9296 - recall: 0.9159 - val_loss: 1.4590 - val_acc: 0.7150 - val_recall: 0.7029\n",
      "Epoch 17/20\n",
      "5s - loss: 0.1905 - acc: 0.9339 - recall: 0.9206 - val_loss: 1.5293 - val_acc: 0.7131 - val_recall: 0.7021\n",
      "Epoch 18/20\n",
      "5s - loss: 0.1818 - acc: 0.9366 - recall: 0.9258 - val_loss: 1.5474 - val_acc: 0.7136 - val_recall: 0.7027\n",
      "Epoch 19/20\n",
      "5s - loss: 0.1737 - acc: 0.9391 - recall: 0.9287 - val_loss: 1.5343 - val_acc: 0.7095 - val_recall: 0.6991\n",
      "Epoch 20/20\n",
      "5s - loss: 0.1661 - acc: 0.9419 - recall: 0.9313 - val_loss: 1.6351 - val_acc: 0.7115 - val_recall: 0.7014\n",
      "Accurcay_mlp_sub_cat: 88.30%\n",
      "Recall_mlp_sub_cat: 87.74%\n"
     ]
    }
   ],
   "source": [
    "# Simple MLP, SGD and Dropout [21 sub_categories]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=5000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(21, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])\n",
    "\n",
    "# sgd = SGD(lr=0.04, decay=1e-6, momentum=0.6, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy', recall])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model for y = categories\n",
    "model.fit(np.array(train.x), train.z, nb_epoch=20, batch_size=32, verbose=2, validation_data=(np.array(val.x), val.z))\n",
    "\n",
    "scores_mlp = model.evaluate(np.array(test.x), test.z, verbose=0)\n",
    "print(\"Accurcay_mlp_sub_cat: %.2f%%\" % (scores_mlp[1]*100))\n",
    "print(\"Recall_mlp_sub_cat: %.2f%%\" % (scores_mlp[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1400, 32)          16000     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 69,503\n",
      "Trainable params: 69,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 18000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      " 1152/18000 [>.............................] - ETA: 457s - loss: 1.0866 - acc: 0.4106 - recall: 0.6814"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d8c2ae3ca13e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM Model [3 categories]\n",
    "\n",
    "top_words = 500\n",
    "embedding_vecor_length = 32\n",
    "max_converse_length = 500\n",
    "# max_converse_length = health_data.converse.map(len).max() # This gives the max length of the converse, used 500 here\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=1400))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(np.array(train.x), train.y, nb_epoch=10, batch_size=64, validation_data=(np.array(validate.x), validate.y))\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(np.array(test.x), test.y, verbose=0)\n",
    "print(\"Accuracy_lstm_cat: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Recall_lstm_cat: %.2f%%\" % (scores[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM Model [21 categories]\n",
    "\n",
    "# top_words = 500\n",
    "# embedding_vecor_length = 32\n",
    "# max_converse_length = 500\n",
    "# # max_converse_length = health_data.converse.map(len).max() # This gives the max length of the converse, used 500 here\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(top_words, embedding_vecor_length, input_length=500))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dense(21, activation='sigmoid'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])\n",
    "# print(model.summary())\n",
    "\n",
    "# model.fit(np.array(train.x), train.z, nb_epoch=1, batch_size=64, validation_data=(np.array(val.x), val.z))\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(np.array(test.x), test.z, verbose=0)\n",
    "# print(\"Accuracy_lstm_sub_cat: %.2f%%\" % (scores[1]*100))\n",
    "# print(\"Recall_lstm_sub_cat: %.2f%%\" % (scores[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 1D \n",
    "\n",
    "# set parameters:\n",
    "max_features = 500\n",
    "maxlen = 1400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10 #Models computational time is a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Convolution 1D model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1400, 50)          25000     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1400, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1398, 250)         37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 753       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 126,253\n",
      "Trainable params: 126,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfsroot/data/home/s_jaysetty/.local/lib/python2.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(250, 3, padding=\"valid\", strides=1, activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "# CNN 1D [3 categories]\n",
    "\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "print('Build Convolution 1D model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 border_mode='valid',\n",
    "                 activation='relu',\n",
    "                 subsample_length=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 8s - loss: 0.5914 - acc: 0.7303 - recall: 0.1734 - val_loss: 1.3650 - val_acc: 0.4250 - val_recall: 0.3150\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 7s - loss: 0.4518 - acc: 0.8038 - recall: 0.1626 - val_loss: 2.2774 - val_acc: 0.3662 - val_recall: 0.3178\n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 7s - loss: 0.4269 - acc: 0.8149 - recall: 0.1879 - val_loss: 3.1005 - val_acc: 0.3387 - val_recall: 0.3262\n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.4132 - acc: 0.8196 - recall: 0.2393 - val_loss: 4.3311 - val_acc: 0.3315 - val_recall: 0.3262\n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.4065 - acc: 0.8206 - recall: 0.2596 - val_loss: 5.0334 - val_acc: 0.3347 - val_recall: 0.3262\n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.3984 - acc: 0.8266 - recall: 0.2358 - val_loss: 4.9216 - val_acc: 0.3363 - val_recall: 0.3260\n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.4006 - acc: 0.8259 - recall: 0.2864 - val_loss: 5.4329 - val_acc: 0.3343 - val_recall: 0.3260\n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.3925 - acc: 0.8282 - recall: 0.2225 - val_loss: 6.3290 - val_acc: 0.3418 - val_recall: 0.3260\n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.3910 - acc: 0.8282 - recall: 0.2204 - val_loss: 6.1687 - val_acc: 0.3537 - val_recall: 0.3260\n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.3785 - acc: 0.8372 - recall: 0.2087 - val_loss: 6.7315 - val_acc: 0.3318 - val_recall: 0.3260\n",
      "Accuracy_cnn1d_cat: 34.48%\n",
      "Recall_cnn1d_cat: 33.75%\n"
     ]
    }
   ],
   "source": [
    "model.fit(np.array(train.x), train.y, batch_size=batch_size, nb_epoch=epochs, validation_data=(np.array(validate.x), validate.y) )\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(np.array(test.x), test.y, verbose=0)\n",
    "print(\"Accuracy_cnn1d_cat: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Recall_cnn1d_cat: %.2f%%\" % (scores[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNN 1D [21 categories]\n",
    "\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# print('Build Convolution 1D model...')\n",
    "# model = Sequential()\n",
    "\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(max_features,\n",
    "#                     embedding_dims,\n",
    "#                     input_length=maxlen))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # we add a Convolution1D, which will learn filters\n",
    "# # word group filters of size filter_length:\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  border_mode='valid',\n",
    "#                  activation='relu',\n",
    "#                  subsample_length=1))\n",
    "# # we use max pooling:\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# # We add a vanilla hidden layer:\n",
    "# model.add(Dense(hidden_dims))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# model.add(Dense(21))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(np.array(train.x), train.z, batch_size=batch_size, nb_epoch=epochs, validation_data=(np.array(val.x), val.z) )\n",
    "\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(np.array(test.x), test.z, verbose=0)\n",
    "# print(\"Accuracy_cnn1d_sub_cat: %.2f%%\" % (scores[1]*100))\n",
    "# print(\"Recall_cnn1d_sub_cat: %.2f%%\" % (scores[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# model_mlp = model.fit(np.array(train.x), train.z, nb_epoch=20, batch_size=32, verbose=2, validation_data=(np.array(val.x), val.z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_mlp\n",
    "# # list all data in history\n",
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
